from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("BQ_Optimized_Read") \
    .config("spark.dynamicAllocation.enabled", "true") \  # Autoscaling enabled
    .config("spark.dynamicAllocation.minExecutors", "10") \  # Minimum executors
    .config("spark.dynamicAllocation.maxExecutors", "50") \  # Maximum executors (auto scale)
    .config("spark.sql.files.maxPartitionBytes", "256MB") \  # Partition size tuning
    .config("spark.sql.shuffle.partitions", "2000") \  # Ensures enough shuffle partitions
    .config("spark.sql.adaptive.enabled", "true") \  # Enable AQE for better performance
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \  # Adjust partitions dynamically
    .config("spark.sql.adaptive.coalescePartitions.minPartitionSize", "64MB") \  # Avoid too small partitions
    .getOrCreate()
