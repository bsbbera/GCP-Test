from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.window import Window
from datetime import datetime, timedelta
import json

# Constants
BQ_PROJECT = "your_project"
BQ_DATASET = "your_dataset"
RESULTS_TABLE = f"{BQ_PROJECT}.{BQ_DATASET}.dq_results"

def generate_bq_query(table_name, numerical_cols, date_col, start_date, end_date):
    """Generate optimized BigQuery SQL for data extraction"""
    cols_str = ", ".join(numerical_cols + [date_col])
    return f"""
        SELECT {cols_str}
        FROM `{BQ_PROJECT}.{BQ_DATASET}.{table_name}`
        WHERE {date_col} BETWEEN '{start_date}' AND '{end_date}'
    """

def process_table(spark, table_config, date_params):
    today, yesterday, start_date_30d = date_params
    table_name = table_config["table_name"]
    
    if not table_config["NUMERICAL_COLS"]:
        print(f"Skipping {table_name} - no numerical columns configured")
        return None
    
    date_col = table_config["DATE_COLS"][0] if table_config["DATE_COLS"] else None
    if not date_col:
        print(f"Skipping {table_name} - no date column found")
        return None
    
    # Generate optimized query
    bq_query = generate_bq_query(
        table_name=table_name,
        numerical_cols=table_config["NUMERICAL_COLS"],
        date_col=date_col,
        start_date=start_date_30d,
        end_date=today
    )
    
    # Execute query (only reads needed columns and rows)
    df = spark.read.format("bigquery") \
        .option("query", bq_query) \
        .load()
    
    # Create daily aggregates (same as before)
    agg_exprs = [F.sum(col).alias(f"{col}_sum") for col in table_config["NUMERICAL_COLS"]]
    df_daily = df.groupBy(date_col).agg(*agg_exprs)
    
    # Melt to long format
    kpi_cols = [f"{col}_sum" for col in table_config["NUMERICAL_COLS"]]
    stack_expr = f"stack({len(kpi_cols)}, {', '.join([f'\"{c}\", `{c}`' for c in kpi_cols])}) as (kpi_name, daily_value)"
    df_long = df_daily.select(date_col, F.expr(stack_expr))
    
    # Window calculations
    kpi_window = Window.partitionBy("kpi_name").orderBy(F.col(date_col).desc())
    rolling_window = Window.partitionBy("kpi_name") \
        .orderBy(F.col(date_col).cast("long")) \
        .rangeBetween(-30*86400, -86400)
    
    # Calculate metrics
    df_metrics = df_long \
        .withColumn("prev_value", F.lag("daily_value").over(kpi_window)) \
        .withColumn("pct_diff", (F.col("daily_value") - F.col("prev_value")) / F.col("prev_value") * 100) \
        .withColumn("mean_30d", F.avg("daily_value").over(rolling_window)) \
        .withColumn("stddev_30d", F.stddev("daily_value").over(rolling_window)) \
        .withColumn("z_score", (F.col("daily_value") - F.col("mean_30d")) / F.col("stddev_30d")) \
        .filter(F.col(date_col) == today) \
        .withColumn("pct_diff_flag", F.when(
            (F.abs(F.col("pct_diff")) > config["global_threshold_pct"]) & 
            F.col("prev_value").isNotNull(), 1).otherwise(0)) \
        .withColumn("sigma_flag", F.when(
            F.abs(F.col("z_score")) > config["global_sigma_threshold"], 1).otherwise(0)) \
        .withColumn("table_name", F.lit(table_name))
    
    return df_metrics.select(
        "table_name",
        "kpi_name",
        F.col(date_col).alias("snapshot_date"),
        "daily_value",
        "prev_value",
        "pct_diff",
        "pct_diff_flag",
        "mean_30d",
        "stddev_30d",
        "z_score",
        "sigma_flag"
    )

def main():
    spark = SparkSession.builder \
        .appName("AutoDataQuality") \
        .config("spark.jars.packages", "com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.28.0") \
        .getOrCreate()
    
    with open("dq_config.json") as f:
        config = json.load(f)
    
    today = datetime.now().strftime("%Y-%m-%d")
    start_date_30d = (datetime.now() - timedelta(days=31)).strftime("%Y-%m-%d")
    date_params = (today, (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d"), start_date_30d)
    
    results = []
    for table_config in config["tables"]:
        try:
            df_result = process_table(spark, table_config, date_params)
            if df_result:
                results.append(df_result)
        except Exception as e:
            print(f"Error processing {table_config.get('table_name', 'unknown')}: {str(e)}")
    
    if results:
        final_df = results[0]
        for df in results[1:]:
            final_df = final_df.union(df)
        
        final_df.write \
            .format("bigquery") \
            .option("table", RESULTS_TABLE) \
            .mode("append") \
            .save()

if __name__ == "__main__":
    main()
