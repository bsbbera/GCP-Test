from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StructField, StringType, TimestampType
import sys
from datetime import datetime
from logger import setup_logger
from config import *
import logging

# Initialize logger
logger = setup_logger()

# Suppress all default Python logging
logging.getLogger().setLevel(logging.ERROR)
# Suppress Spark logging
logging.getLogger('py4j').setLevel(logging.ERROR)
logging.getLogger('pyspark').setLevel(logging.ERROR)

def create_spark_session():
    """Create and configure Spark session"""
    try:
        spark = (SparkSession.builder
                .config("spark.jars", "gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar")
                # Suppress Spark logging to console
                .config("spark.ui.showConsoleProgress", "false")
                .config("spark.ui.enabled", "false")
                .config("spark.sql.execution.arrow.pyspark.enabled", "true"))
        
        # Add configurations from config file
        for key, value in SPARK_CONFIG.items():
            spark = spark.config(key, value)
        
        spark_session = spark.getOrCreate()
        
        # Set Spark log level
        spark_session.sparkContext.setLogLevel("ERROR")
        
        return spark_session
        
    except Exception as e:
        logger.error(f"Failed to create Spark session: {str(e)}")
        sys.exit(1)

def read_from_bigquery(spark):
    """Read data from BigQuery source table"""
    try:
        logger.info(f"Starting to read data from {DATASET_RDL}.{SOURCE_TABLE}")
        
        query = f"""
        SELECT *
        FROM `{PROJECT_ID}.{DATASET_RDL}.{SOURCE_TABLE}`
        WHERE DATE(transaction_date) = '{PROCESS_DATE}'
        """
        
        df = (spark.read.format('bigquery')
             .option('query', query)
             .load())
        
        logger.info(f"Successfully read {df.count()} records from source table")
        return df
        
    except Exception as e:
        logger.error(f"Error reading from BigQuery: {str(e)}")
        raise

def transform_data(df):
    """Transform the data according to business logic"""
    try:
        logger.info("Starting data transformation")
        
        # Add your transformation logic here
        transformed_df = df.groupBy('column_name') \
                         .agg(F.count('*').alias('count'),
                              F.sum('amount').alias('total_amount'))
        
        logger.info("Data transformation completed successfully")
        return transformed_df
        
    except Exception as e:
        logger.error(f"Error during data transformation: {str(e)}")
        raise

def write_to_bigquery(df):
    """Write transformed data to BigQuery target table"""
    try:
        logger.info(f"Starting to write data to {DATASET_REPORTING}.{TARGET_TABLE}")
        
        # Write to BigQuery
        (df.write.format('bigquery')
         .option('table', f'{PROJECT_ID}:{DATASET_REPORTING}.{TARGET_TABLE}')
         .option('temporaryGcsBucket', 'your-temp-bucket')
         .mode('append')
         .save())
        
        logger.info("Successfully wrote data to target table")
        
    except Exception as e:
        logger.error(f"Error writing to BigQuery: {str(e)}")
        raise

def main():
    """Main function to execute the ETL pipeline"""
    start_time = datetime.now()
    logger.info(f"Starting ETL pipeline for date: {PROCESS_DATE}")
    
    try:
        # Initialize Spark
        spark = create_spark_session()
        
        # Extract
        source_df = read_from_bigquery(spark)
        
        # Transform
        transformed_df = transform_data(source_df)
        
        # Load
        write_to_bigquery(transformed_df)
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        logger.info(f"ETL pipeline completed successfully. Duration: {duration} seconds")
        
    except Exception as e:
        logger.error(f"ETL pipeline failed: {str(e)}")
        sys.exit(1)
    finally:
        spark.stop()

if __name__ == "__main__":
    main()
